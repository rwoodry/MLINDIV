{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Functional Connectivity Trial-by-Trial Classifier\n",
    "### Given the functional connectivity of a subject during a specific trial, can we predict the Subject's trial accuracy? \n",
    "Here we extract ROI timeseries as from each time window that encapsualtes the fMRI bold data from a specific trial. For a full set of test runs, this should yield 48 ROI Timeseries of varying length (dependent on how long each trial took to complete). For each ROI Timeseries we compute a functional connectivity matrix, called a 'connectome' using various kinds of funcitonal connectivity metrics. We then feed the Trial-by-Trial Connectomes and their respective trial accuracy labels (0 for incorrect or 1 for correct) into a Support Vector Machine Classifier with a linear kernel. We hold out some of the data for cross validation and test this model over several randomized cross-validation splits. We test the model classifier for each functional connectivity metric for each cross-validation split, and plot their average performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nilearn\n",
    "from nilearn import datasets\n",
    "from nilearn.input_data import NiftiLabelsMasker\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "from nilearn import plotting\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import pandas\n",
    "import os\n",
    "import glob\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from nilearn import plotting\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define 'Helper' Functions\n",
    "The following functions are used to help with preprocessing the Functional Bold data into the Time-Windowed ROI Timeseries with respective accuracy labels. This serves as the model's X and Y values, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Trial Labels\n",
    "From behavioral data, create a pandas data frame containing accuracy labels and respective run names (1-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trial_labels(subjectnum, sync_pulses, test_trials):\n",
    "    # Filter by subject\n",
    "    test_labels = test_trials[test_trials['Subject'] == subjectnum].copy()\n",
    "    \n",
    "    # Initialize Pandas data series for Start and End TRs, as well as for fmri_run number\n",
    "    start_TR = pandas.Series(dtype = 'float32')\n",
    "    end_TR = pandas.Series(dtype = 'float32')\n",
    "    fmri_run = pandas.Series(dtype = 'float32')\n",
    "    \n",
    "    # Get order of task type for labeling wich fmri run each entry corresponds too\n",
    "    time_indices = np.unique(test_labels['times'], return_index = True)[1]\n",
    "    task_order = []\n",
    "    [task_order.append(test_labels.iloc[[i]]['Task_type'].to_string().split()[1]) for i in time_indices]\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    for i in sync_pulses.index:\n",
    "\n",
    "        data = test_labels[test_labels['Task_type'] == sync_pulses.trial_type[i]].copy()\n",
    "\n",
    "        trial_times = np.unique\n",
    "        start_tr = np.floor((data[\"trial_starttime\"].copy() - sync_pulses.getready_response[i]) / 720).astype(int)\n",
    "        end_tr = np.floor((data[\"trial_endtime\"].copy() - sync_pulses.getready_response[i]) / 720).astype(int)\n",
    "        run_num = task_order.index(sync_pulses.trial_type[i])\n",
    "\n",
    "\n",
    "        start_TR = pandas.concat([start_TR, pandas.Series(start_tr)])\n",
    "        end_TR = pandas.concat([end_TR, pandas.Series(end_tr)])\n",
    "        \n",
    "        if run_num > 1:\n",
    "            run_num = 'Run' + str(run_num - 1)\n",
    "        elif run_num == 1:\n",
    "            run_num = 'Ex2' \n",
    "        else:\n",
    "            run_num = 'Ex'\n",
    "        fmri_run = pandas.concat([fmri_run, pandas.Series([run_num] * len(end_tr))])\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    test_labels = test_labels[['Subject', 'Task_type', 'trial_starttime', 'trial_endtime', 'accuracy']]\n",
    "    \n",
    "    test_labels['start_TR'] = start_TR.values\n",
    "    test_labels['end_TR'] = end_TR.values\n",
    "    test_labels['fmri_run'] = fmri_run.values\n",
    "\n",
    "    \n",
    "    return test_labels, task_order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract ROI Timeseries\n",
    "Given a funcitonal scan and atlas ROI map, extract bold signal into ROI time series. Smooth and standardize the signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subject_timeseries(mlindiv_filename, confounds_filename, atlas_filename):\n",
    "    \n",
    "    # Load fmri volumes\n",
    "    func_img = nib.load(mlindiv_filename)\n",
    "    header_zooms = func_img.header.get_zooms()\n",
    "    TR = header_zooms[3]\n",
    "\n",
    "    pandas.read_csv(confounds_filename, sep = '\\t').fillna(0).to_csv(\"conf_nona_temp.tsv\", sep = '\\t', index = False)\n",
    "\n",
    "    print('Atlas ROIs are located in nifti image (4D) at: %s' % atlas_filename)\n",
    "    print('Func images are located in nifti image (4D) at: %s \\n\\t--- Confounds at: %s' % (mlindiv_filename, confounds_filename))\n",
    "    print('Func images Voxel Dimensions (mm): %s\\tFunc TR: %s' % (header_zooms[0:3], header_zooms[3]))\n",
    "\n",
    "    # Create a masker from the HO atlas\n",
    "    masker = NiftiLabelsMasker(\n",
    "        labels_img = atlas_filename, \n",
    "        standardize=True, \n",
    "        memory = 'nilearn_cache', \n",
    "        verbose = 5, low_pass=0.08, high_pass=0.009, detrend=True, t_r = TR)\n",
    "\n",
    "\n",
    "    # Create an overall time series for the run\n",
    "    time_series = masker.fit_transform(mlindiv_filename, confounds = 'conf_nona_temp.tsv')\n",
    "    \n",
    "    return time_series\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Trial-by-Trial ROI Timeseries\n",
    "Given an ROI Timeseries from a signle scan, bin timeseries into timewindows bounded by the begin and end trial times for each trial during that scan run. Thus, we take an ROI time series from a signle run and split it into 8 separate ROI time series corresponding to the 8 individual trials for that scan run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timewindows(timeseries, trial_labels, task_label, start_padding = 0, end_padding = 0):\n",
    "    t_labels = trial_labels[trial_labels['Task_type'] == task_label]\n",
    "    \n",
    "    time_window_num = t_labels.shape[0]\n",
    "\n",
    "    \n",
    "    time_windows = []\n",
    "\n",
    "    for i in range(time_window_num):\n",
    "        \n",
    "        start_TR = t_labels.start_TR.iloc[i] + start_padding\n",
    "        end_TR = t_labels.end_TR.iloc[i] + end_padding\n",
    "        \n",
    "        time_window = timeseries[start_TR:end_TR, :]\n",
    "        time_windows.append(time_window)\n",
    "        \n",
    "    return time_windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate Trial-by-Trial ROI Timeseries\n",
    "For each scan run, use the previously defined functions to aggregate all Trial-by-Trial ROI Timeseries into one dictionary where each key is the Task label / Scan of Origin (i.e. 'A2' or 'C5') and its values are a list of that scan's Trial-by-Trial ROI Timeseries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_timewindows(subject, func_directory, confounds_directory, trial_labels, atlas_filename, start_padding = 0, end_padding = 0):\n",
    "    \n",
    "    # Define list of files to be processed, given subject number and location of functional files.\n",
    "    directory_path = \"%s/sub-%s\" % (func_directory, subject)\n",
    "    trial_filenames = os.listdir(directory_path)\n",
    "    print(\"Functional Images for Subject %s stored in %s\" % (subject, directory_path))\n",
    "    print(\"Functional files to be processed: \" + str(trial_filenames))\n",
    "    \n",
    "    # Initialize empty Dictionary of Windowed - Time Series\n",
    "    dict_timewindows = {}\n",
    "    \n",
    "    for run in trial_filenames:\n",
    "        # Grab task label from matching the func run number to trial labels.\n",
    "        run_name = run.split(\"bold\")[1].split(\"_\")[0]\n",
    "        task_label = np.unique(trial_labels[trial_labels['fmri_run'] == run_name].Task_type)[0]\n",
    "        print(\"Computing ROI Time Series for %s: %s\" % (run_name, task_label))\n",
    "        \n",
    "        # Get path to func file and corresponding confounds file\n",
    "        mlindiv_filename = directory_path+ '/' + run\n",
    "        confounds_filename = glob.glob(\"%s/*%s*%s_*.tsv\" % (confounds_directory, subject, run_name))[0]\n",
    "        \n",
    "        # Compute time series for func run\n",
    "        time_series = subject_timeseries(mlindiv_filename, confounds_filename, atlas_filename)\n",
    "        \n",
    "        # Split time series into list time windows\n",
    "        time_windows = timewindows(time_series, trial_labels, task_label, start_padding, end_padding)\n",
    "        \n",
    "        # For current Run, assign the list of time windows obtained from the previous line\n",
    "        dict_timewindows[task_label] = time_windows\n",
    "    \n",
    "    return dict_timewindows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model the SVM Classifier\n",
    "Using the trial-by-trial ROI timeseries, compute their functional connectomes and fit the trial accuracy for each trial to its connectome. This is done over several cross-validation splits and using different kinds of functional connectivity measures to fit the model. If plot_scores is set to True, plot the average performance for each model (by functional connectivity measure used). The default functional connectivity measures to use are: \n",
    "- Correlation\n",
    "- Partial Correlation\n",
    "- Tangent\n",
    "- Covariance\n",
    "- Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_connectome(ts_list, ts_labels, C = 1, n_splits = 10, fconn_types = ['correlation', 'partial correlation', 'tangent', 'covariance', 'precision'], plot_scores = True):\n",
    "    # If ts_list | ts_labels are a dictionary, collapse them into a list\n",
    "    if type(ts_labels) == dict:\n",
    "        ts_list = list(ts_list.values())[0]\n",
    "        ts_labels = list(ts_labels.values())[0]\n",
    "        \n",
    "    # Define and run the Model\n",
    "    _, classes = np.unique(ts_labels, return_inverse=True)  # Convert accuracy into numpy array of binary labels\n",
    "    ts_array = np.asarray(ts_list) # Convert list of time series into a numpy array of time series\n",
    "\n",
    "    # Define correlation types\n",
    "    kinds = fconn_types\n",
    "    _, classes = np.unique(ts_labels, return_inverse=True)\n",
    "    cv = StratifiedShuffleSplit(n_splits=n_splits, random_state=0, test_size=0.3)\n",
    "    ts_array = np.asarray(ts_list)\n",
    "\n",
    "    scores = {}\n",
    "\n",
    "    for kind in kinds:\n",
    "        scores[kind] = []\n",
    "        for train, test in cv.split(ts_array, classes):\n",
    "            connectivity = ConnectivityMeasure(kind = kind, vectorize = True) # We vectorize the Functional Connectivity, so it is easier to run the SVC classifier on\n",
    "            connectomes = connectivity.fit_transform(ts_array[train])\n",
    "\n",
    "            classifier = LinearSVC(C = C).fit(connectomes, classes[train])\n",
    "            predictions = classifier.predict(connectivity.transform(ts_array[test]))\n",
    "            scores[kind].append(accuracy_score(classes[test], predictions))\n",
    "    \n",
    "    if plot_scores:\n",
    "        mean_scores = [np.mean(scores[kind]) for kind in kinds]\n",
    "        scores_std = [np.std(scores[kind]) for kind in kinds]\n",
    "    \n",
    "        plt.figure(figsize=(6, 4))\n",
    "        positions = np.arange(len(kinds)) * .1 + .1\n",
    "        plt.barh(positions, mean_scores, align='center', height=.05, xerr=scores_std)\n",
    "        yticks = [k.replace(' ', '\\n') for k in kinds]\n",
    "        plt.yticks(positions, yticks)\n",
    "        plt.gca().grid(True)\n",
    "        plt.gca().set_axisbelow(True)\n",
    "        plt.gca().axvline(.7, color='red', linestyle='--')\n",
    "        plt.xlabel('Classification accuracy\\n(red line = chance level)')\n",
    "        plt.tight_layout()\n",
    "\n",
    "    return(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dictionary of Subjects and their Trial-by-Trial ROI Timeseries with Accuracy Labels\n",
    "Using the helper functions defined above, for any given list of subjects and the correct directories for reading in certain files:\n",
    "- create a dictionary of subjects as keys with ROI Timeseries as values \n",
    "- create a dictionary of subjects as keys with accuracy labels as values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subject_timeseries_labels(sub_num_list, behav_directory, func_directory, confounds_directory, sync_pulses_directory, atlas_filename):\n",
    "    '''\n",
    "    This function takes a list of subject numbers and runs through several helper functions iterating over each subject\n",
    "    to output a dictionary of subject - timeseries pairs (X) and a dictionary of subject - labels pairs (Y) to be run through any model.\n",
    "        Requirements:\n",
    "            - A behavioral directory containing the trial master.csv\n",
    "            - a func directory pointing to the functional files to be used. Make sure there are no duplicate runs\n",
    "            - a confoudns directory contianing only conformatted .tsv files (refer to R script conformat)\n",
    "            - a directory containing syncpulse.csv files for each subject\n",
    "    It takes roughly 45 secs to extract and properly label each ROI time series per Test Scan. So for scans looking at 6 runs it can take a little under 4 min to fully run. \n",
    "    It does store extractions in a cache, however, so when rerun using previously run subjects it should be a lot quicker.\n",
    "    '''\n",
    "    # Read in trial data\n",
    "    test_trials = pandas.read_csv('%s/MLINDIV_trial_master.csv' % behav_directory)\n",
    "    \n",
    "    subject_timeseries = {}\n",
    "    subject_labels = {}\n",
    "    \n",
    "    # Loop through each subject's and create ROI timeseries for each trial\n",
    "    for sub_num in sub_num_list:\n",
    "        # Read in the sync_pulse start time data\n",
    "        sync_pulses = pandas.read_csv('%s/sub-%s_syncpulses.csv' % (sync_pulses_directory, sub_num))\n",
    "        \n",
    "        # Create trial labels for the subject, using the sync pulse trial start time info\n",
    "        trial_labels, _ = create_trial_labels(int(sub_num), sync_pulses, test_trials)\n",
    "        \n",
    "        # Extract ROI time series into a trial dictionary (keys = Run name, values = list of ROI time series for that run) for each trial timewindow defined by the trial labels\n",
    "        sub_TW = aggregate_timewindows(sub_num, func_directory, confounds_directory, trial_labels, atlas_filename)\n",
    "        \n",
    "        ts_list = []\n",
    "        ts_labels = []\n",
    "        \n",
    "        # Iterate through Subject's Trial dictionary, creating a List of timeseries (ts_list) and a list of accuracy labels for those timeseries irrespective of run type\n",
    "        for tasktype in sub_TW.keys():\n",
    "            if not tasktype.isnumeric():      \n",
    "                acc = list(trial_labels[trial_labels['Task_type'] == tasktype]['accuracy'])\n",
    "                [ts_labels.append(value) for value in acc]\n",
    "                for ts in sub_TW[tasktype]:\n",
    "                    ts_list.append(ts)   \n",
    "                    \n",
    "        subject_timeseries[sub_num] = ts_list\n",
    "        subject_labels[sub_num] = ts_labels\n",
    "        \n",
    "        return subject_timeseries, subject_labels\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create X and Y Dictionaries\n",
    "First define what atlas to extract ROIs into. Then create dictionaries of extracted & binned ROI timeseries that will be fed into the SVM model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functional Images for Subject 043 stored in D:/SampleData/func/sub-043\n",
      "Functional files to be processed: ['sub-043_task-boldEx_run-1_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz', 'sub-043_task-boldRun1_run-1_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz', 'sub-043_task-boldRun2_run-1_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz', 'sub-043_task-boldRun3_run-1_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz', 'sub-043_task-boldRun4_run-2_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz', 'sub-043_task-boldRun5_run-1_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz', 'sub-043_task-boldRun6_run-1_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz']\n",
      "Computing ROI Time Series for Ex: 1\n",
      "Atlas ROIs are located in nifti image (4D) at: C:\\Users\\17868/nilearn_data\\fsl\\data\\atlases\\HarvardOxford\\HarvardOxford-sub-maxprob-thr25-2mm.nii.gz\n",
      "Func images are located in nifti image (4D) at: D:/SampleData/func/sub-043/sub-043_task-boldEx_run-1_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz \n",
      "\t--- Confounds at: D:/SampleData/confounds\\sub-043_task-boldEx_run-1_conformatted.tsv\n",
      "Func images Voxel Dimensions (mm): (2.0, 2.0, 2.0)\tFunc TR: 0.72\n",
      "[NiftiLabelsMasker.fit_transform] loading data from C:\\Users\\17868/nilearn_data\\fsl\\data\\atlases\\HarvardOxford\\HarvardOxford-sub-maxprob-thr25-2mm.nii.gz\n",
      "Resampling labels\n",
      "[Memory]0.7s, 0.0min    : Loading filter_and_extract...\n",
      "__________________________________filter_and_extract cache loaded - 0.0s, 0.0min\n",
      "Computing ROI Time Series for Run1: A5\n",
      "Atlas ROIs are located in nifti image (4D) at: C:\\Users\\17868/nilearn_data\\fsl\\data\\atlases\\HarvardOxford\\HarvardOxford-sub-maxprob-thr25-2mm.nii.gz\n",
      "Func images are located in nifti image (4D) at: D:/SampleData/func/sub-043/sub-043_task-boldRun1_run-1_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz \n",
      "\t--- Confounds at: D:/SampleData/confounds\\sub-043_task-boldRun1_run-1_conformatted.tsv\n",
      "Func images Voxel Dimensions (mm): (2.0, 2.0, 2.0)\tFunc TR: 0.72\n",
      "[NiftiLabelsMasker.fit_transform] loading data from C:\\Users\\17868/nilearn_data\\fsl\\data\\atlases\\HarvardOxford\\HarvardOxford-sub-maxprob-thr25-2mm.nii.gz\n",
      "Resampling labels\n",
      "[Memory]0.4s, 0.0min    : Loading filter_and_extract...\n",
      "__________________________________filter_and_extract cache loaded - 0.0s, 0.0min\n",
      "Computing ROI Time Series for Run2: A6\n",
      "Atlas ROIs are located in nifti image (4D) at: C:\\Users\\17868/nilearn_data\\fsl\\data\\atlases\\HarvardOxford\\HarvardOxford-sub-maxprob-thr25-2mm.nii.gz\n",
      "Func images are located in nifti image (4D) at: D:/SampleData/func/sub-043/sub-043_task-boldRun2_run-1_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz \n",
      "\t--- Confounds at: D:/SampleData/confounds\\sub-043_task-boldRun2_run-1_conformatted.tsv\n",
      "Func images Voxel Dimensions (mm): (2.0, 2.0, 2.0)\tFunc TR: 0.72\n",
      "[NiftiLabelsMasker.fit_transform] loading data from C:\\Users\\17868/nilearn_data\\fsl\\data\\atlases\\HarvardOxford\\HarvardOxford-sub-maxprob-thr25-2mm.nii.gz\n",
      "Resampling labels\n",
      "[Memory]0.3s, 0.0min    : Loading filter_and_extract...\n",
      "__________________________________filter_and_extract cache loaded - 0.0s, 0.0min\n",
      "Computing ROI Time Series for Run3: A4\n",
      "Atlas ROIs are located in nifti image (4D) at: C:\\Users\\17868/nilearn_data\\fsl\\data\\atlases\\HarvardOxford\\HarvardOxford-sub-maxprob-thr25-2mm.nii.gz\n",
      "Func images are located in nifti image (4D) at: D:/SampleData/func/sub-043/sub-043_task-boldRun3_run-1_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz \n",
      "\t--- Confounds at: D:/SampleData/confounds\\sub-043_task-boldRun3_run-1_conformatted.tsv\n",
      "Func images Voxel Dimensions (mm): (2.0, 2.0, 2.0)\tFunc TR: 0.72\n",
      "[NiftiLabelsMasker.fit_transform] loading data from C:\\Users\\17868/nilearn_data\\fsl\\data\\atlases\\HarvardOxford\\HarvardOxford-sub-maxprob-thr25-2mm.nii.gz\n",
      "Resampling labels\n",
      "[Memory]0.3s, 0.0min    : Loading filter_and_extract...\n",
      "__________________________________filter_and_extract cache loaded - 0.0s, 0.0min\n",
      "Computing ROI Time Series for Run4: A1\n",
      "Atlas ROIs are located in nifti image (4D) at: C:\\Users\\17868/nilearn_data\\fsl\\data\\atlases\\HarvardOxford\\HarvardOxford-sub-maxprob-thr25-2mm.nii.gz\n",
      "Func images are located in nifti image (4D) at: D:/SampleData/func/sub-043/sub-043_task-boldRun4_run-2_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz \n",
      "\t--- Confounds at: D:/SampleData/confounds\\sub-043_task-boldRun4_run-2_conformatted.tsv\n",
      "Func images Voxel Dimensions (mm): (2.0, 2.0, 2.0)\tFunc TR: 0.72\n",
      "[NiftiLabelsMasker.fit_transform] loading data from C:\\Users\\17868/nilearn_data\\fsl\\data\\atlases\\HarvardOxford\\HarvardOxford-sub-maxprob-thr25-2mm.nii.gz\n",
      "Resampling labels\n",
      "[Memory]0.3s, 0.0min    : Loading filter_and_extract...\n",
      "__________________________________filter_and_extract cache loaded - 0.0s, 0.0min\n",
      "Computing ROI Time Series for Run5: A2\n",
      "Atlas ROIs are located in nifti image (4D) at: C:\\Users\\17868/nilearn_data\\fsl\\data\\atlases\\HarvardOxford\\HarvardOxford-sub-maxprob-thr25-2mm.nii.gz\n",
      "Func images are located in nifti image (4D) at: D:/SampleData/func/sub-043/sub-043_task-boldRun5_run-1_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz \n",
      "\t--- Confounds at: D:/SampleData/confounds\\sub-043_task-boldRun5_run-1_conformatted.tsv\n",
      "Func images Voxel Dimensions (mm): (2.0, 2.0, 2.0)\tFunc TR: 0.72\n",
      "[NiftiLabelsMasker.fit_transform] loading data from C:\\Users\\17868/nilearn_data\\fsl\\data\\atlases\\HarvardOxford\\HarvardOxford-sub-maxprob-thr25-2mm.nii.gz\n",
      "Resampling labels\n",
      "[Memory]0.3s, 0.0min    : Loading filter_and_extract...\n",
      "__________________________________filter_and_extract cache loaded - 0.0s, 0.0min\n",
      "Computing ROI Time Series for Run6: A3\n",
      "Atlas ROIs are located in nifti image (4D) at: C:\\Users\\17868/nilearn_data\\fsl\\data\\atlases\\HarvardOxford\\HarvardOxford-sub-maxprob-thr25-2mm.nii.gz\n",
      "Func images are located in nifti image (4D) at: D:/SampleData/func/sub-043/sub-043_task-boldRun6_run-1_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz \n",
      "\t--- Confounds at: D:/SampleData/confounds\\sub-043_task-boldRun6_run-1_conformatted.tsv\n",
      "Func images Voxel Dimensions (mm): (2.0, 2.0, 2.0)\tFunc TR: 0.72\n",
      "[NiftiLabelsMasker.fit_transform] loading data from C:\\Users\\17868/nilearn_data\\fsl\\data\\atlases\\HarvardOxford\\HarvardOxford-sub-maxprob-thr25-2mm.nii.gz\n",
      "Resampling labels\n",
      "[Memory]0.3s, 0.0min    : Loading filter_and_extract...\n",
      "__________________________________filter_and_extract cache loaded - 0.0s, 0.0min\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = datasets.fetch_atlas_harvard_oxford('sub-maxprob-thr25-2mm')\n",
    "atlas_filename = dataset.maps\n",
    "labels = dataset.labels\n",
    "\n",
    "\n",
    "sub_X_dict, sub_Y_dict = subject_timeseries_labels(sub_num_list = ['043'], \n",
    "                                                   behav_directory = 'D:/SampleData',\n",
    "                                                   func_directory = 'D:/SampleData/func',\n",
    "                                                   confounds_directory = 'D:/SampleData/confounds',\n",
    "                                                   sync_pulses_directory = 'D:/SampleData',\n",
    "                                                   atlas_filename = atlas_filename\n",
    "                                                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Model\n",
    "Using the X and Y dictionaries, compute several iterations of the SVM model and store the scores for average model performance (for each functional connectivity measure used). Plot the scores if plot_scores is set to True (default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores = model_connectome(sub_X_dict['043'], sub_Y_dict['043'], C = 0.005, n_splits = 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(48 * .7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we see that even when training on a small number of trial connectomes (70% of 48 trials (rounded down) = 33 trials to train on), some models perform slightly better than chance. Will the model improve given more training data (using multiple subjects)? Some things that could improve the model:\n",
    "   - More training trials (i.e. use more subjects)\n",
    "   - Use a Cortical atlas & Subcortical Atlas for more ROIs across the whole brain\n",
    "   - OR Use Dictionary Learning on MLINDIV subjects Resting State fMRI to build an experiment specific functional ROI Atlas\n",
    "   - Binarize the connectomes according to some threshold (i.e. top 20% of conenctions). This might help the model learn faster as well as serve as some form of regularization, as it creates sparsity across features.\n",
    "   - Test alternate time window sizes. Shift time windows 6+ TRs to capture HRF delay, or widen windows so that each time window is of the same duration centered at the midpoint of trial runtime.\n",
    "   - Try different Model Classifiers: Logistic Regression, SVM with a Gaussian Kernel, etc.\n",
    "   - MAYBE: Try Deep Learning Model architectures: Fully Connected Layers, Sequence to Sequence RNNs, Unidirectional LSTM, Bidirectional LSTM, A Combination of Several of these.\n",
    "\n",
    "If Succesful:\n",
    "- Move on to Regression Models to predict Subject Performance\n",
    "- Use Dynamic Functional Connectivity\n",
    "- Use DNNs, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
